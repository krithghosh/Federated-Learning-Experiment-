{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q tensorflow==2.0.0-beta1\n",
    "!pip install --quiet  tf-nightly\n",
    "!pip install -q h5py pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, models, layers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Loading the MNIST dataset\n",
    "''' \n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "\n",
    "train_images, train_labels = train_images[:30000], train_labels[:30000]\n",
    "test_images, test_labels = test_images[:9000], test_labels[:9000]\n",
    "train_images, test_images = train_images/255.0, test_images/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Building and compiling the model, considered loss as sparse_categorical_crossentropy since our output is an integer.\n",
    "Conv2D(32) -> MaxPooling2D -> Conv2D(64) -> MaxPooling2D -> Flatten(3D to 1D) -> Dense(64) -> Dense(10)\n",
    "''' \n",
    "\n",
    "def build_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Training the model over the given parameters.\n",
    "''' \n",
    "\n",
    "def fit_model(model, train_images, train_labels, epochs):\n",
    "    model.fit(train_images, train_labels, epochs=epochs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Evaluating and printing the accuracy.\n",
    "''' \n",
    "\n",
    "def test_model(model, test_images, test_labels):\n",
    "    test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "    print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Creating three kind of dataset, each having half the dataset of the overall one.\n",
    "'''\n",
    "\n",
    "train_images_1, train_labels_1 = train_images[:5000], train_labels[:5000]\n",
    "train_images_2, train_labels_2 = train_images[5000:10000], train_labels[5000:10000]\n",
    "train_images_3, train_labels_3 = train_images[10000:15000], train_labels[10000:15000]\n",
    "\n",
    "test_images_1, test_labels_1 = test_images[:3000], test_labels[:3000]\n",
    "test_images_2, test_labels_2 = test_images[3000:6000], test_labels[3000:6000]\n",
    "test_images_3, test_labels_3 = test_images[6000:9000], test_labels[6000:9000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_compiled_model(train_images, train_labels, epochs):\n",
    "    model = build_model()\n",
    "    model = fit_model(model, train_images, train_labels, epochs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 28, 28, 1) (5,)\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "Picking first 5 data to train the model, such that it has initial weights.\n",
    "''' \n",
    "\n",
    "initial_train_images = train_images[:5]\n",
    "initial_train_labels = train_labels[:5]\n",
    "\n",
    "print(initial_train_images.shape, initial_train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5 samples\n",
      "Epoch 1/5\n",
      "5/5 [==============================] - 0s 19ms/sample - loss: 2.3382 - accuracy: 0.0000e+00\n",
      "Epoch 2/5\n",
      "5/5 [==============================] - 0s 999us/sample - loss: 2.2385 - accuracy: 0.4000\n",
      "Epoch 3/5\n",
      "5/5 [==============================] - 0s 1ms/sample - loss: 2.1707 - accuracy: 0.8000\n",
      "Epoch 4/5\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 2.0999 - accuracy: 0.8000\n",
      "Epoch 5/5\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 2.0147 - accuracy: 0.6000\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Initial model is being trained, this model would be serialized and sent over to client\n",
    "where it would be retrained on different dataset and sent back to the server.\n",
    "'''\n",
    "\n",
    "initial_model = get_compiled_model(initial_train_images, initial_train_labels, 5)\n",
    "initial_model.save('initial_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000/9000 [==============================] - 1s 123us/sample - loss: 2.2705 - accuracy: 0.1964\n",
      "0.19644445\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "Testing the initial model, initial accuracy of 20% approx.\n",
    "'''\n",
    "\n",
    "test_model(initial_model, test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Considering we have two clients, each receive the same initial model.\n",
    "''' \n",
    "\n",
    "initial_model_1 = models.load_model('initial_model.h5')\n",
    "initial_model_2 = models.load_model('initial_model.h5')\n",
    "initial_model_3 = models.load_model('initial_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5000 samples\n",
      "Epoch 1/2\n",
      "5000/5000 [==============================] - 2s 381us/sample - loss: 0.6375 - accuracy: 0.8100\n",
      "Epoch 2/2\n",
      "5000/5000 [==============================] - 2s 373us/sample - loss: 0.1713 - accuracy: 0.9502\n",
      "Train on 5000 samples\n",
      "Epoch 1/2\n",
      "5000/5000 [==============================] - 2s 395us/sample - loss: 0.6510 - accuracy: 0.7990\n",
      "Epoch 2/2\n",
      "5000/5000 [==============================] - 2s 347us/sample - loss: 0.1903 - accuracy: 0.9428\n",
      "Train on 5000 samples\n",
      "Epoch 1/2\n",
      "5000/5000 [==============================] - 2s 408us/sample - loss: 0.6688 - accuracy: 0.7880\n",
      "Epoch 2/2\n",
      "5000/5000 [==============================] - 2s 350us/sample - loss: 0.1966 - accuracy: 0.9364\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xb326930f0>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "Training the initial model over the new dataset, we won't be compiling the model, only fit the model.\n",
    "''' \n",
    "\n",
    "# Client 1:\n",
    "initial_model_1.fit(train_images_1, train_labels_1, epochs=2)\n",
    "\n",
    "# Client 2:\n",
    "initial_model_2.fit(train_images_2, train_labels_2, epochs=2)\n",
    "\n",
    "# Client 3:\n",
    "initial_model_3.fit(train_images_3, train_labels_3, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000/9000 [==============================] - 1s 112us/sample - loss: 0.1364 - accuracy: 0.9590\n",
      "0.959\n",
      "9000/9000 [==============================] - 1s 110us/sample - loss: 0.1525 - accuracy: 0.9529\n",
      "0.9528889\n",
      "9000/9000 [==============================] - 1s 108us/sample - loss: 0.1483 - accuracy: 0.9571\n",
      "0.9571111\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Testing the models over entire dataset, The highest accuracy obtained is 95.90% and the lowest is 95.29%.\n",
    "'''\n",
    "test_model(initial_model_1, test_images, test_labels)\n",
    "test_model(initial_model_2, test_images, test_labels)\n",
    "test_model(initial_model_3, test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Averaging the weights as done in the paper:\n",
    "Communication-Efficient Learning of Deep Networks from Decentralized Data - H.Brendan McMahan, \n",
    "Eider Moore, Daniel Ramage, Seth Hampson, Blaise Agu Ìˆera y Arcas.\n",
    "'''\n",
    "resultant_weights = np.add(initial_model_1.weights, initial_model_2.weights)\n",
    "resultant_weights = np.add(resultant_weights, initial_model_3.weights)\n",
    "resultant_weights = resultant_weights/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000/9000 [==============================] - 1s 106us/sample - loss: 2.2705 - accuracy: 0.1964\n",
      "0.19644445\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Loading the initial model and testing on the overall test data. The accuracy obtained is 15%, \n",
    "Next we would set the averaged weights from two models and test it again.\n",
    "'''\n",
    "\n",
    "resultant_model = models.load_model('initial_model.h5')\n",
    "test_model(resultant_model, test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000/9000 [==============================] - 1s 102us/sample - loss: 0.1260 - accuracy: 0.9644\n",
      "0.96444446\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Setting the averaged weights and testing on the overall test data. The accuracy obtained is 96.44%.\n",
    "'''\n",
    "\n",
    "resultant_model.set_weights(resultant_weights)\n",
    "test_model(resultant_model, test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
